{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": []
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "code",
      "execution_count": 103,
      "metadata": {
        "id": "XfjPvmwUIoRI"
      },
      "outputs": [],
      "source": [
        "class EventsTagger:\n",
        "  def __init__(self, path_to_file):\n",
        "    # imports and dependencies\n",
        "    import numpy as np\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    import re\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    with open(path_to_file, \"rb\") as f:\n",
        "      text = f.read().decode(\"utf-8\")\n",
        "    self.paragraph = text\n",
        "    self.keywords = ['academic', 'computer science', 'humanities', 'fellowship', 'protest', 'social justice', 'workshop', 'seminar', 'conference', 'symposium', 'panel', 'lecture', 'workshop series', 'career fair', 'networking', 'cultural', 'social', 'club', 'volunteer', 'orientation', 'recruitment', 'sports', 'party', 'wellness']\n",
        "    self.W1 = np.load('W1.npy')\n",
        "    self.V = np.load('V.npy')\n",
        "    self.m = len(self.V) # n_terms\n",
        "    self.topic_names_dict = {1: \"community events and workshops\",\n",
        "                             2: \"social good and fundraising\",\n",
        "                             3: \"cultural and artistic events\",\n",
        "                             4: \"career development\",\n",
        "                             5: \"volunteering\",\n",
        "                             6: \"theater and performing arts\",\n",
        "                             7: \"recreation and nightlife\",\n",
        "                             8: \"pre-professional events\",\n",
        "                             9: \"social justice and advocacy\",\n",
        "                             10: \"food and snacks\",\n",
        "                             11: \"other\"}\n",
        "\n",
        "  # cleaning a single event description\n",
        "  def clean_desc(self, desc):\n",
        "    import re\n",
        "    clean_desc = desc.lower()\n",
        "    clean_desc = re.sub('[^a-zA-Z]', ' ', clean_desc)\n",
        "    clean_desc=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",clean_desc)\n",
        "    clean_desc=re.sub(\"(\\\\d|\\\\W)+\",\" \",clean_desc)\n",
        "    clean_desc = clean_desc.replace('\\n', '')\n",
        "    clean_desc = clean_desc.replace('\\Terminate\\\\', '\\n')\n",
        "    clean_desc = clean_desc.replace('Subject: ', '')\n",
        "    return clean_desc\n",
        "\n",
        "  # cleaning the whole text\n",
        "  def clean_text(self):\n",
        "    import nltk\n",
        "    nltk.download('punkt')\n",
        "    from nltk.tokenize import sent_tokenize\n",
        "    sentences = sent_tokenize(self.paragraph)\n",
        "    self.clean_txt = []\n",
        "    for sentence in sentences:\n",
        "      desc = self.clean_desc(sentence)\n",
        "      self.clean_txt.append(desc)\n",
        "\n",
        "  # fitting the model\n",
        "  def fit(self):\n",
        "    import gensim\n",
        "    from gensim.models import Word2Vec\n",
        "    self.clean_text()\n",
        "    corpus = []\n",
        "    for col in self.clean_txt:\n",
        "      word_list = col.split(\" \")\n",
        "      corpus.append(word_list)\n",
        "    self.word2vec_model = Word2Vec(corpus, min_count=1, vector_size = 70)\n",
        "\n",
        "    # create vector embeddings for keywords\n",
        "    self.keyword_embeddings = {}\n",
        "    for keyword in self.keywords:\n",
        "      keyword_tokens = keyword.split()  # Tokenize the keyword if it contains multiple words\n",
        "      keyword_embedding = []\n",
        "      for token in keyword_tokens:\n",
        "        if token in self.word2vec_model.wv.key_to_index:\n",
        "            keyword_embedding.append(self.word2vec_model.wv.get_vector(token))\n",
        "      if keyword_embedding:\n",
        "        self.keyword_embeddings[keyword] = sum(keyword_embedding) / len(keyword_embedding)\n",
        "\n",
        "  # tags events\n",
        "  def tag(self, event):\n",
        "    # all the helper functions are subfunctions so that we only need one thing\n",
        "\n",
        "    def clean_event(desc):\n",
        "      import re # !!!!!\n",
        "      clean_desc = desc.lower()\n",
        "      clean_desc = re.sub('[^a-zA-Z]', ' ', clean_desc)\n",
        "      clean_desc=re.sub(\"&lt;/?.*?&gt;\",\" &lt;&gt; \",clean_desc)\n",
        "      clean_desc=re.sub(\"(\\\\d|\\\\W)+\",\" \",clean_desc)\n",
        "      clean_desc = clean_desc.replace('\\n', '')\n",
        "      clean_desc = clean_desc.replace('\\Terminate\\\\', '\\n')\n",
        "      clean_desc = clean_desc.replace('Subject: ', '')\n",
        "      return clean_desc\n",
        "\n",
        "    # gets similarities between keywords\n",
        "    def get_keyword_similarities(event):\n",
        "      event_description = clean_event(event).split()\n",
        "\n",
        "      # calculate similarity between event description and each keyword\n",
        "      similarity_scores = {}\n",
        "      for keyword, embedding in self.keyword_embeddings.items(): #!!!!!\n",
        "        keyword_similarity = []\n",
        "        for word in event_description:\n",
        "          if word in self.word2vec_model.wv.key_to_index: #!!!!!\n",
        "            word_embedding = self.word2vec_model.wv.get_vector(word)\n",
        "            similarity = self.word2vec_model.wv.cosine_similarities(embedding, [word_embedding])\n",
        "            keyword_similarity.append(max(similarity))\n",
        "            if keyword_similarity:\n",
        "              similarity_scores[keyword] = max(keyword_similarity)\n",
        "\n",
        "      sorted_keywords = sorted(similarity_scores.items(), key=lambda x: x[1], reverse=True)\n",
        "      top_keywords = sorted_keywords[:10]\n",
        "      return top_keywords\n",
        "\n",
        "    def vectorize_doc(cleaned_doc):\n",
        "      word_to_index = {word: index for index, word in enumerate(self.V)} #!!!!\n",
        "      word_counts = {}\n",
        "      words = cleaned_doc.split()\n",
        "      for word in words:\n",
        "        if word in word_to_index:\n",
        "          word_index = word_to_index[word]\n",
        "          word_counts[word_index] = word_counts.get(word_index, 0) + 1\n",
        "\n",
        "      document_vector = [word_counts.get(index, 0) for index in range(len(self.V))]\n",
        "      return document_vector\n",
        "\n",
        "    def list_topics_above_proportion(topic_distribution, threshold):\n",
        "      topics_above_threshold = []\n",
        "\n",
        "      for topic_index, proportion in enumerate(topic_distribution):\n",
        "        if proportion > threshold:\n",
        "            topics_above_threshold.append(topic_index + 1)\n",
        "      return topics_above_threshold\n",
        "\n",
        "    def get_topic_numbers(cleaned_doc, threshold=0.3):\n",
        "      import numpy as np\n",
        "      document_vector = vectorize_doc(cleaned_doc)\n",
        "      document_topic_distribution = np.dot(document_vector, self.W1) #!!!!\n",
        "      topics_above_threshold = list_topics_above_proportion(document_topic_distribution, threshold)\n",
        "      topics_sorted_by_proportion = sorted(topics_above_threshold, key=lambda topic_index: document_topic_distribution[topic_index - 1], reverse=True)\n",
        "      return topics_sorted_by_proportion\n",
        "\n",
        "    def get_topics(doc):\n",
        "      cleaned_doc = clean_event(doc)\n",
        "      topic_numbers = get_topic_numbers(cleaned_doc)\n",
        "      tags = [self.topic_names_dict.get(topic) for topic in topic_numbers] #!!!\n",
        "      if tags == []:\n",
        "        return []\n",
        "      else:\n",
        "        return tags\n",
        "\n",
        "    tags = []\n",
        "    keyword_similarities = get_keyword_similarities(event)\n",
        "    for (keyword, score) in keyword_similarities:\n",
        "      if score < 0.97:\n",
        "        break\n",
        "      else:\n",
        "        tags.append(keyword)\n",
        "\n",
        "    clean_event_desc = clean_event(event)\n",
        "    custom_keywords = {\n",
        "        'culture': [\"aamp\", \"obsa\", \"apida\", \"ideas\", \"chaplains\", \"samp\", \"christian\",\\\n",
        "                    \"first love\", \"chaplains\", \"msa\", \"women\", \"poc\", \"black\",\\\n",
        "                    \"mcalister\", \"jewish\", \"hillel\", \"chicanx\", \"latinx\", \"qrc\",\\\n",
        "                    \"queer\", \"lgbt\", \"bsu\", \"indigenous\", \"fli\", \"trans\", \"ismp\", \\\n",
        "                    \"oldenborg\", \"draper\", \"muslim\"],\n",
        "        'academic': [\"stem\", \"professor\", \"course\", \"registration\", \"registrar\", \\\n",
        "                     \"idpo\", \"financial aid\", \"journal\", \"department\", \"grad\"],\n",
        "        'protest': [\"march\", \"sjp\", \"divest\", \"undercurrents\", \"cswa\", \"workers\"],\n",
        "        'wellness': [\"title ix\", \"cares\", \"lmft\"],\n",
        "        'career': [\"cdo\", \"alum\", \"pcip\", \"internship\", \"prehealth\", \"prelaw\", \\\n",
        "                   \"resume\", \"consulting\", \"handshake\"],\n",
        "        'party': ['walker']\n",
        "    }\n",
        "\n",
        "    for tag, lst in custom_keywords.items():\n",
        "      added_tag = False\n",
        "      for word in lst:\n",
        "        if word in clean_event_desc:\n",
        "          added_tag = True\n",
        "          tags.append(tag)\n",
        "          break\n",
        "      # if we already added the tag no need to continue looking through the words\n",
        "      if added_tag == True:\n",
        "        continue\n",
        "\n",
        "    topics = get_topics(event) # !!\n",
        "\n",
        "\n",
        "    return tags + topics\n",
        "\n",
        "\n",
        "  def save_model(self, filename):\n",
        "    import pickle\n",
        "    self.fit()\n",
        "\n",
        "    model_data = {\n",
        "        'V': self.V,\n",
        "        'W1': self.W1,\n",
        "        'word2vec_model': self.word2vec_model,\n",
        "        'topic_names_dict': self.topic_names_dict,\n",
        "        'keyword_embeddings': self.keyword_embeddings,\n",
        "        'tag_method': self.tag\n",
        "    }\n",
        "\n",
        "    with open(filename, 'wb') as f:\n",
        "        pickle.dump(model_data, f)\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "\n",
        ""
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger = EventsTagger('chirp_output2.txt')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "FkjI9WspNjSV",
        "outputId": "1dc930b5-8327-4e0e-cf0e-f6aba5582613"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.fit()"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "AdHiJNXlN_Ys",
        "outputId": "ed69a0f7-13de-4f58-92df-20c2ce1a8e84"
      },
      "execution_count": 105,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.save_model('p-5cevents-tagger3.pkl')"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MrL5gxp5Gcvz",
        "outputId": "011f0238-dcdd-4119-f7a8-e0e3faf266a1"
      },
      "execution_count": 106,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "[nltk_data] Downloading package punkt to /root/nltk_data...\n",
            "[nltk_data]   Package punkt is already up-to-date!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "tagger.tag(\"Come talk to the Computer Science professors in the pannel for understanding computer science course registration\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Y2aWsOUhwEVf",
        "outputId": "692371d6-5173-4ff5-a0f9-1658526326e2"
      },
      "execution_count": 107,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['computer science', 'academic', 'volunteering']"
            ]
          },
          "metadata": {},
          "execution_count": 107
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# Open the text file for reading\n",
        "with open('chirp_output2.txt', 'r') as file:\n",
        "    # Read the entire contents of the file\n",
        "    text = file.read()\n",
        "\n",
        "# Remove line breaks from the text\n",
        "text = text.replace('\\n', ' ')\n",
        "text = text.replace('\\Terminate\\\\', '\\n')\n",
        "text = text.replace('Subject: ', '')\n",
        "\n",
        "\n",
        "# Open the same text file for writing\n",
        "with open('input.txt', 'w') as file:\n",
        "    # Write the modified text to the file\n",
        "    file.write(text)"
      ],
      "metadata": {
        "id": "WSkCulmWwcuM"
      },
      "execution_count": 108,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with open('input.txt', 'r') as input_file, open('output.txt', 'w') as output_file:\n",
        "    for line in input_file:\n",
        "        line = line.strip()\n",
        "        tags = tagger.tag(line)\n",
        "\n",
        "        output_file.write(f\"{line}\\n\")\n",
        "        output_file.write(f\"Tags: {', '.join(tags)}\\n\")\n",
        "        output_file.write(\"\\n\")"
      ],
      "metadata": {
        "id": "rDT0MijUweF6"
      },
      "execution_count": 110,
      "outputs": []
    }
  ]
}